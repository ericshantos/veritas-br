{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0A2psQ9wkKv_"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#@title The MIT License (MIT)\n",
        "#\n",
        "# Copyright (c) 2025 Eric dos Santos.\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__vplneBkhrq"
      },
      "source": [
        "# Fake News Classification\n",
        "\n",
        "This project aims to develop a neural network for detecting fake news in Portuguese, using the dataset [Fake.br-Corpus](https://github.com/roneysco/Fake.br-Corpus). With this, we seek to create a system capable of identifying patterns and distinguishing fake news from real news, contributing to the fight against misinformation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbMFa6CEBg_M"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"center\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/ericshantos/br_fake_news_detector/blob/main/br_fake_news_detector_model.ipynb\n",
        "\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run on Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/ericshantos/br_fake_news_detector/blob/main/br_fake_news_detector_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View the code on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSx8er0qkk-f"
      },
      "source": [
        "## Dataset loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3oc9DOJISWn",
        "outputId": "b0287972-2afd-4c0a-9680-9a7e1af1f6fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Fake.br-Corpus'...\n",
            "remote: Enumerating objects: 28763, done.\u001b[K\n",
            "remote: Total 28763 (delta 0), reused 0 (delta 0), pack-reused 28763 (from 1)\u001b[K\n",
            "Receiving objects: 100% (28763/28763), 37.10 MiB | 11.95 MiB/s, done.\n",
            "Resolving deltas: 100% (14129/14129), done.\n",
            "Updating files: 100% (21602/21602), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/roneysco/Fake.br-Corpus\n",
        "DATA_PATH = \"./Fake.br-Corpus/full_texts\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bZmwUyTf8xcp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# News Directory\n",
        "fake_dir = f\"{DATA_PATH}/fake\"\n",
        "real_dir = f\"{DATA_PATH}/true\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99mmIko9Fwoq"
      },
      "source": [
        "### News content extraction:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eroFrkQJbCMF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def load_news(news_dir: str, label: str) -> pd.DataFrame:\n",
        "    # List to store news\n",
        "    news = []\n",
        "\n",
        "    # Cycle through all files in the specified directory\n",
        "    for filename in os.listdir(news_dir):\n",
        "        # Checks if the file has the .txt extension\n",
        "        if filename.endswith(\".txt\"):\n",
        "            # Gets the full path of the file\n",
        "            file_path = os.path.join(news_dir, filename)\n",
        "\n",
        "            # Open the file and read its contents\n",
        "            with open(file_path, \"r\") as file:\n",
        "                content = file.read()\n",
        "\n",
        "                # Adds the content and label to the news list\n",
        "                news.append({\"text\": content, \"label\": label})\n",
        "\n",
        "    # Returns a pandas DataFrame containing the news\n",
        "    return pd.DataFrame(news)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfJq8ohRb_uz"
      },
      "source": [
        "Result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nm6aTvsicB6f"
      },
      "outputs": [],
      "source": [
        "fake_news = load_news(fake_dir, 0)\n",
        "real_news = load_news(real_dir, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROaBKDzaR8wL"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFMd18ZPTBNz"
      },
      "source": [
        "### Concatenate the DataFrames\n",
        "\n",
        "Group Dataframes to generate a single robust database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "31gJ9BtPVyw8"
      },
      "outputs": [],
      "source": [
        "data_news = pd.concat([fake_news, real_news], ignore_index=True).sample(frac=1, random_state=13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO_NWvm4WnFb"
      },
      "source": [
        "Final base information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwLb0lbYNO0Q",
        "outputId": "779fb1fc-37fd-4333-b6db-0c76fd43a953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 7200 entries, 3248 to 338\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    7200 non-null   object\n",
            " 1   label   7200 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 168.8+ KB\n"
          ]
        }
      ],
      "source": [
        "data_news.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lUB-N1znrzs",
        "outputId": "92332129-2881-47be-e15b-45b6ca6297e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text      object\n",
            "label    float64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "data_news = data_news.apply(\n",
        "\n",
        "    # If valid, type the column as float\n",
        "    lambda col: col.astype(float) if col.apply(\n",
        "\n",
        "        # Check if they are digits\n",
        "        lambda x: str(x).replace('.', '', 1).isdigit()\n",
        "    ).all() else col\n",
        ")\n",
        "\n",
        "# Result\n",
        "print(data_news.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eulKWpHUd7N6"
      },
      "source": [
        "### Data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qSuQr25Vd_Lc"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download pt_core_news_sm > /dev/null 2>&1\n",
        "!pip install unidecode > /dev/null 2>&1\n",
        "\n",
        "from unidecode import unidecode\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "  # Text processing\n",
        "  doc = nlp(text)\n",
        "\n",
        "  # Tokenization, stopword removal, punctuation and accentuation\n",
        "  tokens = [unidecode(token.lemma_) for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "  return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvZtWBPrnrwh"
      },
      "source": [
        "Clear news content:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "L9G4v-2Anwos"
      },
      "outputs": [],
      "source": [
        "data_news[\"text\"] = data_news[\"text\"].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLdX7o7YYw_N",
        "outputId": "e78bdd85-97f2-4529-8736-ab2848d8c1be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 7200 entries, 3248 to 338\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   text    7200 non-null   object \n",
            " 1   label   7200 non-null   float64\n",
            "dtypes: float64(1), object(1)\n",
            "memory usage: 168.8+ KB\n"
          ]
        }
      ],
      "source": [
        "data_news.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuQ3WkgqIsBI"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8OxbRGqSIuh7"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense, Input\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Tokenizer Object\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "\n",
        "tokenizer.fit_on_texts(data_news['text'])\n",
        "\n",
        "# Converting texts into sequences of numbers\n",
        "sequences = tokenizer.texts_to_sequences(data_news['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9Y-s9O-XQWK"
      },
      "source": [
        "### Prepares the labels and data for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SqhrN-rYXbL0"
      },
      "outputs": [],
      "source": [
        "# Transform the texts into sequences of numbers\n",
        "X = pad_sequences(sequences, maxlen=200)\n",
        "\n",
        "# news labels(fake or real)\n",
        "y = data_news[\"label\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6DPLfsfCPy0"
      },
      "source": [
        "### Save the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DpjSJ7--CTa1"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1Ox8QKuaCVHy"
      },
      "outputs": [],
      "source": [
        "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiII8APuYGvQ"
      },
      "source": [
        "### Splitting the dataset into training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJt1LgGsYK7z",
        "outputId": "6087a837-70e7-453a-f76e-e6331f89787c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: (5760, 200)\n",
            "Test set size: (1440, 200)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splits data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape}\")\n",
        "print(f\"Test set size: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYuO9WkZfP7-"
      },
      "source": [
        "### Model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pbD9hBQHfexY"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "\n",
        "    # Input layer\n",
        "    Input(shape=(200,)),\n",
        "\n",
        "    # Convert tokens to dense vectors: input layer\n",
        "    Embedding(input_dim=10000, output_dim=128),\n",
        "    LSTM(128, return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(32),\n",
        "\n",
        "    # Output Layer\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3Cz-_IghY2x"
      },
      "source": [
        "**Model compilation**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "48Xru8lehe4_"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT5eiqtmjGyW"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwONYo0NjKca",
        "outputId": "4d21e265-22cd-488c-b541-5e4c38f39f81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 55ms/step - accuracy: 0.7121 - loss: 0.4965 - val_accuracy: 0.9368 - val_loss: 0.2330\n",
            "Epoch 2/5\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.9412 - loss: 0.2239 - val_accuracy: 0.9368 - val_loss: 0.2352\n",
            "Epoch 3/5\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.9443 - loss: 0.2158 - val_accuracy: 0.9368 - val_loss: 0.2337\n",
            "Epoch 4/5\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9431 - loss: 0.2172 - val_accuracy: 0.9368 - val_loss: 0.2334\n",
            "Epoch 5/5\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9384 - loss: 0.2313 - val_accuracy: 0.9368 - val_loss: 0.2346\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(X_train, y_train, epochs=5, batch_size=128, validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLE2D3OfkfJJ"
      },
      "source": [
        "#### Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0kMz2vHWCvwf"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GnYa3DyCpvX",
        "outputId": "8e831135-7f95-4a34-b813-81241386b41c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"
          ]
        }
      ],
      "source": [
        "# Getting the variances (probabilities)\n",
        "y_pred_proba = model.predict(X_test)\n",
        "\n",
        "# Applying the threshold\n",
        "y_pred = (y_pred_proba >= 0.7).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrsWpj3ICpLM",
        "outputId": "d7a12fd5-9782-4a47-bd2c-0b952e60dbd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 Score:  0.9345794392523364\n"
          ]
        }
      ],
      "source": [
        "print(f\"F1 Score: \", f1_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiUG0Ae3Exx2",
        "outputId": "f0ee44f6-1f90-4b00-f089-aff4ce842f44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.9368055555555556\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuracy: \", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVMqhYgADQPL",
        "outputId": "d8f5cba4-4304-4c59-bcf4-ce97fe6fb683"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision:  0.9558823529411765\n"
          ]
        }
      ],
      "source": [
        "print(f\"Precision: \", precision_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6m1fCOJDUvQ",
        "outputId": "0be0ec74-a64d-4c3f-ab66-ac27a8bffb4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recall:  0.9142053445850914\n"
          ]
        }
      ],
      "source": [
        "print(f\"Recall: \", recall_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwlcZd7CDWmK",
        "outputId": "750e850d-1870-4370-8044-ce358ec37b33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROC AUC:  0.9588766763325289\n"
          ]
        }
      ],
      "source": [
        "print(f\"ROC AUC: \", roc_auc_score(y_test, y_pred_proba))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hh3xnDMBj-s"
      },
      "source": [
        "### Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "jRHqOJZgBmza"
      },
      "outputs": [],
      "source": [
        "model.save(\"veritas-lstm-v1.1-ptbr.keras\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
